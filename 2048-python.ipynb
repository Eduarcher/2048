{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Initializes a new game with size n\n",
    "def new_game(n):\n",
    "    return np.matrix([[0] * n] * n)\n",
    "\n",
    "\n",
    "# Adds a new tile to the matrix\n",
    "def add_tile(matrix):\n",
    "    n, m = matrix.shape\n",
    "    i = rd.randint(0, n - 1)\n",
    "    j = rd.randint(0, m - 1)\n",
    "\n",
    "    while matrix[i,j] != 0:\n",
    "        i = rd.randint(0, n - 1)\n",
    "        j = rd.randint(0, m - 1)\n",
    "        \n",
    "    matrix[i,j] = 4 if (rd.random() < 0.1) else 2\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Returns whether a game given by a matrix is over or not\n",
    "def is_over(matrix):\n",
    "    m, n = matrix.shape\n",
    "\n",
    "    # Check for empty entries\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if matrix[i,j] == 0:\n",
    "                return False\n",
    "\n",
    "    # Check for left/right entries\n",
    "    for i in range(n):\n",
    "        for j in range(m - 1):\n",
    "            if matrix[i,j] == matrix[i,j+1]:\n",
    "                return False\n",
    "\n",
    "    # Check for up/down entries\n",
    "    for i in range(n - 1):\n",
    "        for j in range(m):\n",
    "            if matrix[i,j] == matrix[i+1,j]:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Move tiles to the left\n",
    "def cover_up(matrix):\n",
    "    n, m = matrix.shape\n",
    "    new = new_game(n)\n",
    "    updated = False\n",
    "    for i in range(n):\n",
    "        count = 0\n",
    "        for j in range(m):\n",
    "            if matrix[i,j] != 0:\n",
    "                new[i,count] = matrix[i,j]\n",
    "                updated = j != count\n",
    "                count += 1\n",
    "    return new, updated\n",
    "\n",
    "\n",
    "# Merge tiles to the left\n",
    "def merge(matrix):\n",
    "    n, m = matrix.shape\n",
    "    updated = False\n",
    "    score = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m - 1):\n",
    "            if matrix[i,j] == matrix[i,j+1] != 0:\n",
    "                matrix[i,j] *= 2\n",
    "                matrix[i,j+1] = 0\n",
    "                updated = True\n",
    "                score += matrix[i,j]\n",
    "    return matrix, updated, score\n",
    "\n",
    "\n",
    "# Simulates an up movement\n",
    "def up(matrix):\n",
    "    matrix = np.rot90(matrix)\n",
    "    matrix, updated = cover_up(matrix)\n",
    "    temp = merge(matrix)\n",
    "    matrix = cover_up(temp[0])[0]\n",
    "    matrix = np.rot90(matrix, 3)\n",
    "    updated = updated or temp[1]\n",
    "    score = temp[2]\n",
    "\n",
    "    return matrix, updated, score\n",
    "\n",
    "\n",
    "# Simulates a down movement\n",
    "def down(matrix):\n",
    "    matrix = np.rot90(matrix, 3)\n",
    "    matrix, updated = cover_up(matrix)\n",
    "    temp = merge(matrix)\n",
    "    matrix = cover_up(temp[0])[0]\n",
    "    matrix = np.rot90(matrix)\n",
    "    updated = updated or temp[1]\n",
    "    score = temp[2]\n",
    "\n",
    "    return matrix, updated, score\n",
    "\n",
    "\n",
    "# Simulates a left movement\n",
    "def left(matrix):\n",
    "    matrix, updated = cover_up(matrix)\n",
    "    temp = merge(matrix)\n",
    "    matrix = cover_up(temp[0])[0]\n",
    "    updated = updated or temp[1]\n",
    "    score = temp[2]\n",
    "\n",
    "    return matrix, updated, score\n",
    "\n",
    "\n",
    "# Simulates a right movement\n",
    "def right(matrix):\n",
    "    matrix = np.rot90(matrix, 2)\n",
    "    matrix, updated = cover_up(matrix)\n",
    "    temp = merge(matrix)\n",
    "    matrix = cover_up(temp[0])[0]\n",
    "    matrix = np.rot90(matrix, 2)\n",
    "    updated = updated or temp[1]\n",
    "    score = temp[2]\n",
    "\n",
    "    return matrix, updated, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "ACTIONS = [up, right, down, left]\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, state, reward, done, score):\n",
    "        self.state = state\n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.score = score\n",
    "\n",
    "\n",
    "# Returns the initial state\n",
    "def initial_state():\n",
    "    state = new_game(4)\n",
    "    state = add_tile(state)\n",
    "    state = add_tile(state)\n",
    "    return Environment(state=state, reward=0, done=False, score=0)\n",
    "\n",
    "\n",
    "# Simulates a action given a environment object\n",
    "def step(action, env):\n",
    "    if not env.done:\n",
    "        state, updated, reward = ACTIONS[action](env.state)\n",
    "        if updated:\n",
    "            state = add_tile(state)\n",
    "            done = is_over(state)\n",
    "            score = env.score + reward\n",
    "            return Environment(state, reward, done, score)\n",
    "        else:\n",
    "            state = env.state\n",
    "            reward = 0\n",
    "            done = True\n",
    "            score = env.score\n",
    "            return Environment(state, reward, done, score)\n",
    "    else:\n",
    "        state = env.state\n",
    "        reward = 0\n",
    "        done = True\n",
    "        score = env.score\n",
    "        return Environment(state, reward, done, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = None\n",
    "        \n",
    "    # Builds the DQN Model\n",
    "    def build_model(self):\n",
    "        # Network Layers\n",
    "        inputs = Input(shape=(self.state_size,), name='inputs')\n",
    "        hidden_1 = Dense(64, activation='relu', name='hidden_1')(inputs)\n",
    "        hidden_2 = Dense(32, activation='relu', name='hidden_2')(hidden_1)\n",
    "        outputs = Dense(self.action_size, activation='linear', name='outputs')(hidden_2)\n",
    "        \n",
    "        # Model\n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate), metrics=['acc'])\n",
    "        self.model.summary()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return rd.randrange(self.action_size)\n",
    "        else:\n",
    "            q_values = self.model.predict(state)[0]\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        batch = rd.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward if (done) else reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            q_values = np.reshape(self.model.predict(state)[0], (1,4))\n",
    "            q_values[0,action] = target\n",
    "            self.model.fit(state, q_values, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "# Train the model of a given agent\n",
    "def train(n_episodes, n_steps, n_replay, agent):\n",
    "    print(f\"Start training for {n_episodes} episodes with {n_steps} steps...\")\n",
    "    for episode in range(n_episodes):\n",
    "        env = initial_state()\n",
    "        for _ in range(n_steps):\n",
    "            action = agent.act(env.state.flatten())\n",
    "            next_env = step(action, env)\n",
    "            \n",
    "            agent.remember(env.state.flatten(), action, next_env.reward, next_env.state.flatten(), next_env.done)\n",
    "            agent.replay(n_replay)\n",
    "            \n",
    "            env = next_env\n",
    "            if env.done:\n",
    "                break\n",
    "                \n",
    "        print(f\"episode: {episode+1}/{n_episodes}, score: {env.score}\")\n",
    "                \n",
    "    print(\"Finished training\")\n",
    "    agent.model.save(\"DQN_Model.h5\")\n",
    "    \n",
    "\n",
    "# Test an agent\n",
    "def test(n_episodes=5000, n_steps=500, n_replay=32, model_path=None):\n",
    "    agent = DQNAgent(state_size=16, action_size=4)\n",
    "    if model_path:\n",
    "        print(f\"Load model from: {model_path}\")\n",
    "        agent.model = load_model(model_path)\n",
    "    else:\n",
    "        agent.build_model()\n",
    "        train(n_episodes, n_steps, n_replay, agent)\n",
    "    env = initial_state()\n",
    "    print(\"Start playing...\")\n",
    "    while not env.done:\n",
    "        action = agent.act(env.state)\n",
    "        env = step(action, env)\n",
    "    print(f\"Finished playing with a score of {env.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "hidden_1 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,300\n",
      "Trainable params: 3,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Start training for 5000 episodes with 500 steps...\n",
      "episode: 1/5000, score: 0\n",
      "episode: 2/5000, score: 4\n",
      "episode: 3/5000, score: 0\n",
      "episode: 4/5000, score: 0\n",
      "episode: 5/5000, score: 0\n",
      "episode: 6/5000, score: 4\n",
      "episode: 7/5000, score: 36\n",
      "episode: 8/5000, score: 0\n",
      "episode: 9/5000, score: 24\n",
      "episode: 10/5000, score: 48\n",
      "episode: 11/5000, score: 4\n",
      "episode: 12/5000, score: 8\n",
      "episode: 13/5000, score: 60\n",
      "episode: 14/5000, score: 36\n",
      "episode: 15/5000, score: 4\n",
      "episode: 16/5000, score: 16\n",
      "episode: 17/5000, score: 0\n",
      "episode: 18/5000, score: 4\n",
      "episode: 19/5000, score: 4\n",
      "episode: 20/5000, score: 52\n",
      "episode: 21/5000, score: 16\n",
      "episode: 22/5000, score: 20\n",
      "episode: 23/5000, score: 68\n",
      "episode: 24/5000, score: 0\n",
      "episode: 25/5000, score: 56\n",
      "episode: 26/5000, score: 4\n",
      "episode: 27/5000, score: 4\n",
      "episode: 28/5000, score: 16\n",
      "episode: 29/5000, score: 4\n",
      "episode: 30/5000, score: 20\n",
      "episode: 31/5000, score: 16\n",
      "episode: 32/5000, score: 4\n",
      "episode: 33/5000, score: 24\n",
      "episode: 34/5000, score: 40\n",
      "episode: 35/5000, score: 100\n",
      "episode: 36/5000, score: 4\n",
      "episode: 37/5000, score: 24\n",
      "episode: 38/5000, score: 0\n",
      "episode: 39/5000, score: 12\n",
      "episode: 40/5000, score: 60\n",
      "episode: 41/5000, score: 8\n",
      "episode: 42/5000, score: 20\n",
      "episode: 43/5000, score: 164\n",
      "episode: 44/5000, score: 0\n",
      "episode: 45/5000, score: 196\n",
      "episode: 46/5000, score: 68\n",
      "episode: 47/5000, score: 60\n",
      "episode: 48/5000, score: 100\n",
      "episode: 49/5000, score: 140\n",
      "episode: 50/5000, score: 0\n",
      "episode: 51/5000, score: 36\n",
      "episode: 52/5000, score: 60\n",
      "episode: 53/5000, score: 16\n",
      "episode: 54/5000, score: 80\n",
      "episode: 55/5000, score: 20\n",
      "episode: 56/5000, score: 8\n",
      "episode: 57/5000, score: 76\n",
      "episode: 58/5000, score: 28\n",
      "episode: 59/5000, score: 20\n",
      "episode: 60/5000, score: 0\n",
      "episode: 61/5000, score: 0\n",
      "episode: 62/5000, score: 0\n",
      "episode: 63/5000, score: 44\n",
      "episode: 64/5000, score: 52\n",
      "episode: 65/5000, score: 64\n",
      "episode: 66/5000, score: 80\n",
      "episode: 67/5000, score: 76\n",
      "episode: 68/5000, score: 0\n",
      "episode: 69/5000, score: 132\n",
      "episode: 70/5000, score: 0\n",
      "episode: 71/5000, score: 4\n",
      "episode: 72/5000, score: 40\n",
      "episode: 73/5000, score: 0\n",
      "episode: 74/5000, score: 4\n",
      "episode: 75/5000, score: 4\n",
      "episode: 76/5000, score: 60\n",
      "episode: 77/5000, score: 0\n",
      "episode: 78/5000, score: 4\n",
      "episode: 79/5000, score: 44\n",
      "episode: 80/5000, score: 72\n",
      "episode: 81/5000, score: 56\n",
      "episode: 82/5000, score: 80\n",
      "episode: 83/5000, score: 28\n",
      "episode: 84/5000, score: 8\n",
      "episode: 85/5000, score: 36\n",
      "episode: 86/5000, score: 0\n",
      "episode: 87/5000, score: 84\n",
      "episode: 88/5000, score: 40\n",
      "episode: 89/5000, score: 64\n",
      "episode: 90/5000, score: 0\n",
      "episode: 91/5000, score: 120\n",
      "episode: 92/5000, score: 72\n",
      "episode: 93/5000, score: 8\n",
      "episode: 94/5000, score: 24\n",
      "episode: 95/5000, score: 20\n",
      "episode: 96/5000, score: 28\n",
      "episode: 97/5000, score: 28\n",
      "episode: 98/5000, score: 204\n",
      "episode: 99/5000, score: 68\n",
      "episode: 100/5000, score: 12\n",
      "episode: 101/5000, score: 60\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "test(5000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}